{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Project Introduction"
      ],
      "metadata": {
        "id": "4AWlL9XR9Qcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Word Prediction using LSTM (Language Modeling)\n",
        "\n",
        "This project builds a resume-grade **Next Word Prediction system**\n",
        "using an LSTM-based language model.\n",
        "\n",
        "Key learning objectives:\n",
        "- Language modeling from first principles\n",
        "- Sliding window sequence generation\n",
        "- Softmax over large vocabularies\n",
        "- Many-to-Many sequence modeling\n",
        "- Foundation for text generation & autocomplete\n"
      ],
      "metadata": {
        "id": "ofr8PlP69Uwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports & Configuration"
      ],
      "metadata": {
        "id": "rDIIdZoD9cjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "W97dSZPh9ddO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "id": "FIPo4X9R9kBA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG (LOCKED)\n",
        "VOCAB_SIZE = 5000\n",
        "CONTEXT_LEN = 5\n",
        "EMBED_DIM = 100\n",
        "LSTM_UNITS = 150\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n"
      ],
      "metadata": {
        "id": "Urvxko5_9mZh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset"
      ],
      "metadata": {
        "id": "8MIyiB9h8EZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAr4jzM-3vF_",
        "outputId": "d1b5595c-71be-4989-eb89-4d564c4ebf04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Total characters: 1115394\n"
          ]
        }
      ],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", url)\n",
        "\n",
        "text = open(path_to_file, \"rb\").read().decode(\"utf-8\")\n",
        "text = text.lower()\n",
        "\n",
        "print(\"Total characters:\", len(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization & Vocabulary"
      ],
      "metadata": {
        "id": "BdgIuTSC9vXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "SHpmQkJJ9wHL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\n",
        "    num_words=VOCAB_SIZE,\n",
        "    oov_token=\"<oov>\",\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "O3GQYjUw9-Ym"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(\"Total unique tokens:\", total_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzuJB_9G-P69",
        "outputId": "7914fbdf-8414-4461-dfa5-725352342fe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique tokens: 12634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Input–Target Sequences (Sliding Window)"
      ],
      "metadata": {
        "id": "aJ_sFGxyFzaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "for i in range(CONTEXT_LEN, len(tokens)):\n",
        "    seq = tokens[i-CONTEXT_LEN:i+1]\n",
        "    sequences.append(seq)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "print(\"Total sequences:\", sequences.shape)\n",
        "\n",
        "print(sequences[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkV9cUuy-dbs",
        "outputId": "4b1a7b32-4836-441f-80ee-041bd817c9e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: (204084, 6)\n",
            "[ 89 270 140  36 970 144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Input & Target"
      ],
      "metadata": {
        "id": "GGKiAV94GaOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n"
      ],
      "metadata": {
        "id": "Ro8jB7i2Gfk_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build LSTM Language Model"
      ],
      "metadata": {
        "id": "9xu03wuDGimm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
      ],
      "metadata": {
        "id": "QX0h6197Gkkv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(VOCAB_SIZE, EMBED_DIM, input_length=CONTEXT_LEN),\n",
        "    LSTM(LSTM_UNITS),\n",
        "    Dense(VOCAB_SIZE, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCV-Ep04Gq1c",
        "outputId": "e835db9e-9fd1-4f45-9043-191ca6491409"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "ZD0P7xMJGs1q",
        "outputId": "c60635c2-3af4-4f2e-9695-bd9a68821044"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile Model (LOSS MATTERS)"
      ],
      "metadata": {
        "id": "23UlJob4G69K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "w6JpbCZBG8kq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "OWbcfNDTHAjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvgzRPTbHBt-",
        "outputId": "0b516894-dcc2-40bc-b8c5-807ea9b111bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.0494 - loss: 6.5947\n",
            "Epoch 2/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.0780 - loss: 5.9583\n",
            "Epoch 3/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.1017 - loss: 5.6794\n",
            "Epoch 4/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.1109 - loss: 5.4787\n",
            "Epoch 5/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.1189 - loss: 5.3112\n",
            "Epoch 6/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.1251 - loss: 5.1578\n",
            "Epoch 7/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1304 - loss: 5.0155\n",
            "Epoch 8/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1372 - loss: 4.8820\n",
            "Epoch 9/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.1429 - loss: 4.7543\n",
            "Epoch 10/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1508 - loss: 4.6303\n",
            "Epoch 11/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1595 - loss: 4.5097\n",
            "Epoch 12/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1700 - loss: 4.3920\n",
            "Epoch 13/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.1810 - loss: 4.2768\n",
            "Epoch 14/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.1928 - loss: 4.1649\n",
            "Epoch 15/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.2069 - loss: 4.0559\n",
            "Epoch 16/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.2208 - loss: 3.9502\n",
            "Epoch 17/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.2355 - loss: 3.8478\n",
            "Epoch 18/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.2496 - loss: 3.7484\n",
            "Epoch 19/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.2639 - loss: 3.6526\n",
            "Epoch 20/20\n",
            "\u001b[1m1595/1595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.2786 - loss: 3.5601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model & Tokenizer"
      ],
      "metadata": {
        "id": "DKvcF2mwHG_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"next_word_lstm.h5\")\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU8R0QB3HIzy",
        "outputId": "c9978b97-17c5-410f-f14c-6c7a234c9834"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Word Prediction Function"
      ],
      "metadata": {
        "id": "zuX_rnjSHOwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(seed_text):\n",
        "    seed_text = seed_text.lower()\n",
        "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    seq = seq[-CONTEXT_LEN:]\n",
        "    seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [seq], maxlen=CONTEXT_LEN, padding=\"pre\"\n",
        "    )\n",
        "\n",
        "    preds = model.predict(seq, verbose=0)[0]\n",
        "\n",
        "    # block <OOV>\n",
        "    oov_index = tokenizer.word_index.get(\"<OOV>\")\n",
        "    if oov_index is not None:\n",
        "        preds[oov_index] = 0\n",
        "\n",
        "    predicted_id = np.argmax(preds)\n",
        "\n",
        "    return tokenizer.index_word.get(predicted_id, \"\")\n"
      ],
      "metadata": {
        "id": "JqhfbhhFHQdR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Model"
      ],
      "metadata": {
        "id": "J9D76UoYHijd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"it is your \"\n",
        "print(\"Next word:\", predict_next_word(seed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfyX_ZgbHhHK",
        "outputId": "533d514f-4cf9-4e40-faf9-8d9d39e47bf5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word: worship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"we are chosen\"\n",
        "print(predict_next_word(seed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y9040o2KFrZ",
        "outputId": "3688bd28-31a5-487b-aa69-3df1b2691611"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"if things go\"\n",
        "print(predict_next_word(seed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhwCx7saMvUQ",
        "outputId": "47b42c7a-b370-44e0-b2e4-a95ffe1506f1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"i am glad to\"\n",
        "print(predict_next_word(seed_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS7S65gyMtHI",
        "outputId": "cac82dfd-ae77-4f69-ede6-687440f26ab7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n"
          ]
        }
      ]
    }
  ]
}