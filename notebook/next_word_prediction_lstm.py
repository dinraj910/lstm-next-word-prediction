# -*- coding: utf-8 -*-
"""next_word_prediction_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s9XNxHQFUZ4gzK1RJnPE6W1dpHwBD7JW

Project Introduction

# Next Word Prediction using LSTM (Language Modeling)

This project builds a resume-grade **Next Word Prediction system**
using an LSTM-based language model.

Key learning objectives:
- Language modeling from first principles
- Sliding window sequence generation
- Softmax over large vocabularies
- Many-to-Many sequence modeling
- Foundation for text generation & autocomplete

Imports & Configuration
"""

import numpy as np
import tensorflow as tf
import pickle

# Reproducibility
tf.random.set_seed(42)
np.random.seed(42)

# CONFIG (LOCKED)
VOCAB_SIZE = 5000
CONTEXT_LEN = 5
EMBED_DIM = 100
LSTM_UNITS = 150
BATCH_SIZE = 128
EPOCHS = 20

"""Download Dataset"""

url = "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt"
path_to_file = tf.keras.utils.get_file("shakespeare.txt", url)

text = open(path_to_file, "rb").read().decode("utf-8")
text = text.lower()

print("Total characters:", len(text))

"""Tokenization & Vocabulary"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(
    num_words=VOCAB_SIZE,
    oov_token="<oov>",
)

tokenizer.fit_on_texts([text])

total_words = len(tokenizer.word_index) + 1
print("Total unique tokens:", total_words)

"""Generate Inputâ€“Target Sequences (Sliding Window)"""

sequences = []

tokens = tokenizer.texts_to_sequences([text])[0]

for i in range(CONTEXT_LEN, len(tokens)):
    seq = tokens[i-CONTEXT_LEN:i+1]
    sequences.append(seq)

sequences = np.array(sequences)
print("Total sequences:", sequences.shape)

print(sequences[0])

"""Split Input & Target"""

X = sequences[:, :-1]
y = sequences[:, -1]

"""Build LSTM Language Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

model = Sequential([
    Embedding(VOCAB_SIZE, EMBED_DIM, input_length=CONTEXT_LEN),
    LSTM(LSTM_UNITS),
    Dense(VOCAB_SIZE, activation="softmax")
])

model.summary()

"""Compile Model (LOSS MATTERS)"""

model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

"""Train Model"""

history = model.fit(
    X,
    y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS
)

"""Save Model & Tokenizer"""

model.save("next_word_lstm.h5")

with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

"""Next Word Prediction Function"""

def predict_next_word(seed_text):
    seed_text = seed_text.lower()
    seq = tokenizer.texts_to_sequences([seed_text])[0]
    seq = seq[-CONTEXT_LEN:]
    seq = tf.keras.preprocessing.sequence.pad_sequences(
        [seq], maxlen=CONTEXT_LEN, padding="pre"
    )

    preds = model.predict(seq, verbose=0)[0]

    # block <OOV>
    oov_index = tokenizer.word_index.get("<OOV>")
    if oov_index is not None:
        preds[oov_index] = 0

    predicted_id = np.argmax(preds)

    return tokenizer.index_word.get(predicted_id, "")

"""Test the Model"""

seed = "it is your "
print("Next word:", predict_next_word(seed))

seed_text = "we are chosen"
print(predict_next_word(seed_text))

seed_text = "if things go"
print(predict_next_word(seed_text))

seed_text = "i am glad to"
print(predict_next_word(seed_text))